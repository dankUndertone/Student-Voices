{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models Using Paragraph Vectors\n",
    "\n",
    "In this notebook we will use Gensim's [word2vec](https://radimrehurek.com/gensim/models/word2vec.html), [doc2vec](https://radimrehurek.com/gensim/models/doc2vec.html) and [fasttext](https://radimrehurek.com/gensim/models/fasttext.html) models to use a neural networks to identify relationships between the words in the text. The existing pre-trained fasttext models were insufficient to accurately retrieve information for the corpus so we must resort to using our own models. We will evaluate these using an simple information retrieval task on a number of selected words. \n",
    "\n",
    "<br>1) First we will run each language model on several different parameters and return the experimental results. \n",
    "<br>2) Next, we will use the best language model to begin putting together topics based on the direct synonyms of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Make the necessary imports:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlo\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\carlo\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Cores: 3\n",
      "Available Cores: 3\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import visuals as vs\n",
    "import glob\n",
    "import modeling_tools as mt\n",
    "import bear_necessities as bn\n",
    "import multiprocessing\n",
    "import lm_analysis_v2 as lma\n",
    "import warnings \n",
    "\n",
    "from importlib import reload\n",
    "lma = reload(lma)\n",
    "vs = reload(vs)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>none below</th>\n",
       "      <th>none above</th>\n",
       "      <th>epochs</th>\n",
       "      <th>vector size</th>\n",
       "      <th>window length</th>\n",
       "      <th>alpha</th>\n",
       "      <th>nickname</th>\n",
       "      <th>negative sample</th>\n",
       "      <th>review length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20000</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'D2V1'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20000</td>\n",
       "      <td>300</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'D2V2'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20000</td>\n",
       "      <td>300</td>\n",
       "      <td>9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'D2V3'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20000</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'D2V4'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20000</td>\n",
       "      <td>400</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'D2V5'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20000</td>\n",
       "      <td>400</td>\n",
       "      <td>9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'D2V6'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20000</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'D2V7'</td>\n",
       "      <td>10.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20000</td>\n",
       "      <td>400</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'D2V8'</td>\n",
       "      <td>10.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20000</td>\n",
       "      <td>400</td>\n",
       "      <td>9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'D2V9'</td>\n",
       "      <td>10.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'FST1'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>300</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'FST2'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>300</td>\n",
       "      <td>9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'FST3'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'FST4'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>400</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'FST5'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>400</td>\n",
       "      <td>9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'FST6'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'FST7'</td>\n",
       "      <td>10.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>400</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'FST8'</td>\n",
       "      <td>10.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>400</td>\n",
       "      <td>9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'FST9'</td>\n",
       "      <td>10.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'W2V1'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'W2V2'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'W2V3'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'W2V4'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>400</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'W2V5'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>400</td>\n",
       "      <td>9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'W2V6'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'W2V7'</td>\n",
       "      <td>10.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>400</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'W2V8'</td>\n",
       "      <td>10.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>400</td>\n",
       "      <td>9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>'W2V9'</td>\n",
       "      <td>10.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    none below  none above  epochs  vector size  window length  alpha  \\\n",
       "0           30         0.5   20000          300              5   0.05   \n",
       "1           30         0.5   20000          300              7   0.05   \n",
       "2           30         0.5   20000          300              9   0.05   \n",
       "3           30         0.5   20000          400              5   0.05   \n",
       "4           30         0.5   20000          400              7   0.05   \n",
       "5           30         0.5   20000          400              9   0.05   \n",
       "6           30         0.5   20000          400              5   0.05   \n",
       "7           30         0.5   20000          400              7   0.05   \n",
       "8           30         0.5   20000          400              9   0.05   \n",
       "9           30         0.5    1000          300              5   0.05   \n",
       "10          30         0.5    1000          300              7   0.05   \n",
       "11          30         0.5    1000          300              9   0.05   \n",
       "12          30         0.5    1000          400              5   0.05   \n",
       "13          30         0.5    1000          400              7   0.05   \n",
       "14          30         0.5    1000          400              9   0.05   \n",
       "15          30         0.5    1000          400              5   0.05   \n",
       "16          30         0.5    1000          400              7   0.05   \n",
       "17          30         0.5    1000          400              9   0.05   \n",
       "18          30         0.5     200          300              5   0.05   \n",
       "19          30         0.5     200          300              7   0.05   \n",
       "20          30         0.5     200          300              9   0.05   \n",
       "21          30         0.5     200          400              5   0.05   \n",
       "22          30         0.5     200          400              7   0.05   \n",
       "23          30         0.5     200          400              9   0.05   \n",
       "24          30         0.5     200          400              5   0.05   \n",
       "25          30         0.5     200          400              7   0.05   \n",
       "26          30         0.5     200          400              9   0.05   \n",
       "\n",
       "   nickname  negative sample  review length  \n",
       "0    'D2V1'              NaN             80  \n",
       "1    'D2V2'              NaN             80  \n",
       "2    'D2V3'              NaN             80  \n",
       "3    'D2V4'              NaN             80  \n",
       "4    'D2V5'              NaN             80  \n",
       "5    'D2V6'              NaN             80  \n",
       "6    'D2V7'             10.0             80  \n",
       "7    'D2V8'             10.0             80  \n",
       "8    'D2V9'             10.0             80  \n",
       "9    'FST1'              NaN             80  \n",
       "10   'FST2'              NaN             80  \n",
       "11   'FST3'              NaN             80  \n",
       "12   'FST4'              NaN             80  \n",
       "13   'FST5'              NaN             80  \n",
       "14   'FST6'              NaN             80  \n",
       "15   'FST7'             10.0             80  \n",
       "16   'FST8'             10.0             80  \n",
       "17   'FST9'             10.0             80  \n",
       "18   'W2V1'              NaN             80  \n",
       "19   'W2V2'              NaN             80  \n",
       "20   'W2V3'              NaN             80  \n",
       "21   'W2V4'              NaN             80  \n",
       "22   'W2V5'              NaN             80  \n",
       "23   'W2V6'              NaN             80  \n",
       "24   'W2V7'             10.0             80  \n",
       "25   'W2V8'             10.0             80  \n",
       "26   'W2V9'             10.0             80  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experimental_configurations = pd.read_csv('language_model_parameters.txt')\n",
    "\n",
    "# Pull in the different model type configurations \n",
    "d2vexp = experimental_configurations[experimental_configurations['nickname'].str.contains('D2V')]\n",
    "fstexp = experimental_configurations[experimental_configurations['nickname'].str.contains('FST')]\n",
    "w2vexp = experimental_configurations[experimental_configurations['nickname'].str.contains('W2V')]\n",
    "\n",
    "experimental_configurations['review length'] = 80\n",
    "\n",
    "experimental_configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data if need be\n",
    "data = bn.decompress_pickle(os.getcwd() + '/data/review_stats.pbz2')\n",
    "\n",
    "# import the ranges (don't need the indices but its how we can get the up to date ranges quickly)\n",
    "range_indices = bn.loosen(os.getcwd() + '/data/by_rating_range.pickle')\n",
    "ranges = list(np.sort(list(range_indices.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec Training**\n",
    "\n",
    "The original word2vec model is a useful benchmark for evaluating information retrieval tasks. It will also help us decide from the other available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b24621dc943d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# we will place a single filter on review length of 80 characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# for doc2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-b24621dc943d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# we will place a single filter on review length of 80 characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# for doc2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "data_configurations = ['A1','E1']\n",
    "\n",
    "for config in data_configurations:\n",
    "    \n",
    "    # load the cleaned text data\n",
    "    text, stem_map, lemma_map, phrase_frequencies = bn.decompress_pickle(os.getcwd()+'/data/cleaned_data/cleaned_docs_'+config+'.pbz2')\n",
    "    \n",
    "    # define the directory where you want to save the models\n",
    "    model_directory = os.getcwd()+'/models/D2V'+config\n",
    "\n",
    "    # first we will create language models for every range \n",
    "    indices = data['Review_Length']\n",
    "    \n",
    "    # we will place a single filter on review length of 80 characters \n",
    "    docs = [text[idx] for idx in indices[indices>80].index][:1000]\n",
    "        \n",
    "    # for doc2vec \n",
    "    d2v_params = experimental_configurations.loc[experimental_configurations['nickname'].str.contains('D2V')]\n",
    "\n",
    "    # for word2vec \n",
    "    w2v_params = experimental_configurations.loc[experimental_configurations['nickname'].str.contains('W2V')]\n",
    "    \n",
    "    # language models are not as sensitive to input as LDA so we do not vary these parameters \n",
    "    none_below = 30 \n",
    "    none_above = 0.5 \n",
    "\n",
    "    # run the doc2vec analysis for the full dataset \n",
    "    lm.run_d2v_analysis(docs, \n",
    "                        d2v_params,\n",
    "                        none_below,\n",
    "                        none_above,\n",
    "                        config+'_full',\n",
    "                        model_directory)\n",
    "        \n",
    "    \n",
    "    # now for each range\n",
    "    for rng in ranges: \n",
    "\n",
    "        # get the rows in the data that you need \n",
    "        indices = data.loc[range_indices[rng],'Review_Length']\n",
    "\n",
    "        # filter the trainin corpus by review length and save the length \n",
    "        docs = [text[idx] for idx in indices[indices>80].index][:1000]\n",
    "        \n",
    "        # run the doc2vec analysis for ranges in the dataset \n",
    "        lm.run_d2v_analysis(docs, \n",
    "                    d2v_params,\n",
    "                    none_below,\n",
    "                    none_above,\n",
    "                    config+'_'+rng,\n",
    "                    model_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5504077622c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for idx in indices[indices>80].index:\n",
    "    text[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           36\n",
       "1           40\n",
       "2           65\n",
       "3           17\n",
       "4          107\n",
       "5          223\n",
       "6          195\n",
       "7          116\n",
       "8          192\n",
       "9           56\n",
       "10         169\n",
       "11         243\n",
       "12          85\n",
       "13          77\n",
       "14         171\n",
       "15         200\n",
       "16         125\n",
       "17          27\n",
       "18          52\n",
       "19          82\n",
       "20          61\n",
       "21          31\n",
       "22          57\n",
       "23          67\n",
       "24          87\n",
       "25          78\n",
       "26          18\n",
       "27          36\n",
       "28          86\n",
       "29         185\n",
       "          ... \n",
       "4884449     29\n",
       "4884450     44\n",
       "4884451    119\n",
       "4884452     30\n",
       "4884453    149\n",
       "4884454     44\n",
       "4884455      9\n",
       "4884456     97\n",
       "4884457    197\n",
       "4884458     41\n",
       "4884459     20\n",
       "4884460     58\n",
       "4884461     59\n",
       "4884462     52\n",
       "4884463     58\n",
       "4884464     11\n",
       "4884465     37\n",
       "4884466     24\n",
       "4884467     92\n",
       "4884468    160\n",
       "4884469     11\n",
       "4884470     97\n",
       "4884471    108\n",
       "4884472     60\n",
       "4884473     14\n",
       "4884474     12\n",
       "4884475     11\n",
       "4884476    153\n",
       "4884477     69\n",
       "4884478     37\n",
       "Name: Review_Length, Length: 4884479, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4863978"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doc2Vec Training**\n",
    "\n",
    "We will be using gensim's doc2vec framework to generate the word vectors for the vocabulary in our corpus. Doc2Vec can run on multiple cores but in order to take full advantage of this functionality we need to use file based training as specified in [this notebook](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Any2Vec_Filebased.ipynb). For this we need to modify our input a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lma.run_d2v_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FastText Training**\n",
    "\n",
    "Alternatively to doc2vec we can use the FastText model. Instead of word vectors, FastText looks at words as character n-grams making it capable of handling words that aren't in the dictionary or are mispelled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lma.run_ft_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Models\n",
    "\n",
    "Now that we've trained our language models we will evaluate their ability to retrieve information. We want to see if these models are capable of identifying synonyms and antonyms that make sense in the context of the given corpus. We use a standard set of evaluation words to test the models performance and evaluate the output manually to select a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First we select the rating range we want to work with set the results file and pull in the evaluation words:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_range = '[85, 95)' # remember, ranges have a space after the comma\n",
    "\n",
    "# set the output file \n",
    "results_file = os.getcwd()+'/results/language_model_experimental_results.xlsx'\n",
    "\n",
    "# open eval words for topics \n",
    "files = glob.glob(os.getcwd() + '/eval_words/stemmed_lemmatized/*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the available language models and save the results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lma.library_eval(results_file, files, corpus_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Seeds\n",
    "\n",
    "With the loaded model we can begin exploring word similarities. We can use this to evaluate model performance (i.e. do the synonyms make sense?). Your goal should be to identify key words that describe similar topics. In the next section we will save all of these words. If you've reviewed the results file and selected a model to use, go ahead and highlight the model and begin building the seeds. \n",
    "\n",
    "**Import the gensim model needed to load your model of choice:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "moc = 'D2V_A1_D2V5'\n",
    "model = Doc2Vec.load(os.getcwd()+'/models/'+moc+'/'+moc.replace('D2V_','D2V_'+corpus_range)+'.model')\n",
    "materials = bn.decompress_pickle(os.getcwd()+'/models/'+moc+'/'+moc.replace('D2V_','materials_'+corpus_range)+'.pbz2')\n",
    "stem_map, lemma_map, phrase_freq, dictionary = materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load all of the existing words in the destination folder so you can check if a words has been used before:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(os.getcwd()+'/data/subject_words/narrow/*.txt')\n",
    "topic_dict = {}\n",
    "registered_topic_words = [] \n",
    "for f in files: \n",
    "    topic_name = f.split('\\\\')[-1].split('.')[0]\n",
    "    topic_dict[topic_name]=[]\n",
    "    for w in open(f,'r').read().split(): \n",
    "        registered_topic_words.append(w)\n",
    "        topic_dict[topic_name].append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***These words will have been stemmed and lemmatized before training the model, enter is below to find the specific spelling and all the words or phrases it pops up in:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  ['think', 'rethink']\n",
      "Phrases:  ['ned_rethink']\n"
     ]
    }
   ],
   "source": [
    "# Look up specific words stems and phrases \n",
    "word = 'think'\n",
    "\n",
    "print('Words: ',[d for d in [w for w in stem_map if word in w] if d in [dictionary[i] for i in dictionary]])\n",
    "print('Phrases: ',[w.replace(\"b'\",'').replace(' ','_').replace(\"'\",'') for w in [str(k) for k in list(phrase_freq.keys())] if word in w]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t for t in topic_dict.keys() if '_primary' not in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the topic and primary lists\n",
    "subject_words = []\n",
    "primary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_name = ''\n",
    "topic_dict[subject_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['partner']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in subject_words if w not in primary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the words you want to list synonyms for. \n",
    "word_list = ['partner']\n",
    "\n",
    "primary += word_list\n",
    "primary = list(set(primary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partner</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>group</td>\n",
       "      <td>0.465183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exact</td>\n",
       "      <td>0.426385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tabl</td>\n",
       "      <td>0.415747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mate</td>\n",
       "      <td>0.398736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>seat</td>\n",
       "      <td>0.382509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>exit</td>\n",
       "      <td>0.376244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flash</td>\n",
       "      <td>0.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>comput</td>\n",
       "      <td>0.365927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>card</td>\n",
       "      <td>0.360510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>borow</td>\n",
       "      <td>0.359624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the following cell block, make a list of the indices above you would like to keep\n",
      "Full list is 11\n",
      "There are 11 unique words\n"
     ]
    }
   ],
   "source": [
    "syns = []\n",
    "sims = [] \n",
    "removals = []\n",
    "for w in word_list: \n",
    "    if w in model.wv:\n",
    "        syns.append(w)\n",
    "        sims.append(1)\n",
    "    \n",
    "        for entry in model.wv.similar_by_word(w):    \n",
    "            syns.append(entry[0])\n",
    "            sims.append(entry[1])\n",
    "    else:\n",
    "        print('removing',w)\n",
    "        removals.append(w)\n",
    "\n",
    "if len(removals)>0:\n",
    "    word_list = [w for w in word_list if w not in removals]\n",
    "\n",
    "table = vs.split_tables(syns,word_list,sims=sims)\n",
    "vs.display_side_by_side(table)  \n",
    "\n",
    "# format the table so the index you create is capable of selecting from it \n",
    "tbl = pd.DataFrame()\n",
    "for t in table:\n",
    "    try: \n",
    "        tbl = tbl.append(t)\n",
    "    except: \n",
    "        tbl = t\n",
    "table = tbl \n",
    "\n",
    "print('In the following cell block, make a list of the indices above you would like to keep')\n",
    "print('Full list is %i' % len(list(tbl['Word'])))\n",
    "print('There are %i unique words' % len(list(set(tbl['Word']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make a list (separated by commas) with the numbers of the words you would like to include\n",
      "['group', 'group_project', 'partner']\n"
     ]
    }
   ],
   "source": [
    "# Write the index numbers of the words you want to include. \n",
    "print('Make a list (separated by commas) with the numbers of the words you would like to include')\n",
    "idx = input('These will be added to the existing list: ')\n",
    "idx = [int(i) for i in idx.split(',')]\n",
    "try: subject_words\n",
    "except: subject_words = [] \n",
    "for i in idx: subject_words += [table.loc[i].values[0]]\n",
    "subject_words = list(set(subject_words))\n",
    "list.sort(subject_words)\n",
    "\n",
    "print(subject_words)\n",
    "for s in subject_words: \n",
    "    if s in registered_topic_words: \n",
    "        for t in topic_dict: \n",
    "            if '_primary' not in t:\n",
    "                if s in topic_dict[t]:\n",
    "                    print(s,'is in',t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_name = 'group_work'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['group', 'group_project', 'partner']\n",
      "['group_project', 'partner', 'group']\n"
     ]
    }
   ],
   "source": [
    "# set a name to save your current list of words\n",
    "#subject_name = 'fun_or_funny'\n",
    "\n",
    "print(subject_words)\n",
    "with open(os.getcwd() + '/data/subject_words/narrow/' + subject_name + '.txt', 'w') as f:\n",
    "    for w in subject_words: \n",
    "        f.write(w)\n",
    "        f.write('\\n')\n",
    "\n",
    "primary = list(set(primary))\n",
    "print(primary)\n",
    "with open(os.getcwd() + '/data/subject_words/narrow/' + subject_name + '_primary.txt', 'w') as f:\n",
    "    for w in primary: \n",
    "        f.write(w)\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
