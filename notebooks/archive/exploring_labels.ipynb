{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Labels \n",
    "\n",
    "We continue our analysis analyzing the language that best predicts the specific labels. We will use the labels generated in the `explore_clean_setup.ipynb` notebook using the likert scale values submitted by reviewers. We will use [sklearns TfidfVectorizer (Term frequencyâ€“inverse document frequency vectorizer)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to identify the words that are most predictive of each label. TFIDF is a method for counting the relative frequency of terms in a corpus and will provide us a straight forward baseline of which words and phrases are most predictive of a given label.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlo\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import bear_necessities as bn\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random \n",
    "from gensim.models import KeyedVectors\n",
    "import modeling_tools_v2 as mt\n",
    "import visuals as vs \n",
    "from importlib import reload\n",
    "# set the random generator seed \n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load the data:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned text data\n",
    "text, stem_map, lemma_map, phrase_frequencies = bn.decompress_pickle(os.getcwd()+'/data/cleaned_data/cleaned_docs_A1.pbz2')\n",
    "\n",
    "# import the labels indices \n",
    "label_dict = bn.decompress_pickle(os.getcwd()+'/data/labeled_indices.pbz2')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity's sake, our labels are binary. We will submit each one to a logistic regression model for classification once they have been vectorized. We would like a balanced sample of 1's and 0's for our labels. Since every label is a minority in the corpus we will randomly sample reviews that are not under the given label. In fact, because we do not know the degree to which two values of the same characteristic (i.e. Easiness 1 & Easiness 2) share the same language, we will also remove all non-5 indices from the pool of available indices for drawing 0's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_dict = {} \n",
    "# for each set of label indices retrieve a sample of indices of equal size that do not share that label. \n",
    "for k in label_dict:\n",
    "    negative_dict[k] = {} \n",
    "    # we will hold all the non-five indices under this characteristic \n",
    "    character_indices = []\n",
    "    for v in label_dict[k]:\n",
    "        # we exclude characteric values of \"5\" because these are considered default and less informative \n",
    "        if v !=5: \n",
    "            character_indices+= label_dict[k][v]\n",
    "    # now that we have collected all the non-five indices under this characteristic we can loop through again and draw 0's \n",
    "    for v in label_dict[k]:\n",
    "        if v!=5: \n",
    "            # add an entry with a random sample from the indices that are not in the character of the size of the label 1's \n",
    "            negative_dict[k][v] = random.sample(list(np.setdiff1d(range(0,len(text)),character_indices)), len(label_dict[k][v]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning for Labeled Reviews\n",
    "\n",
    "After transforming the text into vectors we will use a supervised learning model called [support vector machines](https://scikit-learn.org/stable/modules/svm.html) to predict text labels using the transformed data. For TFIDF we will use the data that was cleaned most extensively. Since our goal is explanation and not prediction there will be no need to separate the data out into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt = reload(mt) \n",
    "vs = reload(vs)\n",
    "\n",
    "# we loop through each index, get the text and create the label vector \n",
    "for k in label_dict: \n",
    "    for v in label_dict[k]:\n",
    "        if v!=5: \n",
    "            # get the positive labeled text \n",
    "            docs = [text[idx] for idx in label_dict[k][v]]            \n",
    "\n",
    "            # get the negative labeled text \n",
    "            docs = docs + [text[idx] for idx in negative_dict[k][1]]\n",
    "\n",
    "            assert len(label_dict[k][v])==len(negative_dict[k][v])\n",
    "            \n",
    "            # create the label vector \n",
    "            labels = np.concatenate([np.ones(len(negative_dict[k][v])),\n",
    "                                     np.zeros(len(negative_dict[k][v]))])\n",
    "            \n",
    "            # train the svm model \n",
    "            results, svm, tfidf_vec = mt.run_svm(docs, \n",
    "                                                 labels)\n",
    "            \n",
    "            # set the title \n",
    "            title = str(k)+': '+str(v)\n",
    "            # set the graph file name \n",
    "            graph = os.getcwd()+'/graphs/TFIDF Graphs/'+title+'.png'\n",
    "            # visualize the top words or phrases that are predictive of each label \n",
    "            plot_coefficients(results['coef'].values, \n",
    "                              results['word'].values, \n",
    "                              title,\n",
    "                              top_features=40,\n",
    "                              save=graph)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revewing with Pre-Trained FastText Models \n",
    "\n",
    "Now we will load a pre-trained fasttext model. A list of pre-trained vectors can be found [here](https://fasttext.cc/docs/en/english-vectors.html). Using the pre-trained FastText models we can check the words highlighted above, find their synonyms and evaluate the adequency of the pre-trained FastText models to review our data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('C:/Users/carlo/Dropbox/Coding/FastText/crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlo\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('layed', 0.8039053082466125),\n",
       " ('laying', 0.7812391519546509),\n",
       " ('lay', 0.771033525466919),\n",
       " ('Laid', 0.7116783857345581),\n",
       " ('lays', 0.7067301869392395),\n",
       " ('Laying', 0.6783792972564697),\n",
       " ('laid.The', 0.5919531583786011),\n",
       " ('groundwork', 0.5819611549377441),\n",
       " ('laided', 0.5602813959121704),\n",
       " ('laid.', 0.5574567914009094)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('laid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText Tutorial \n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    " \n",
    "model = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')\n",
    "print (model.most_similar('desk'))\n",
    " \n",
    "words = []\n",
    "for word in model.vocab:\n",
    "    words.append(word)\n",
    " \n",
    "print(\"Vector components of a word: {}\".format(\n",
    "    model[words[0]]\n",
    "))\n",
    "sentences = [['this', 'is', 'the', 'good', 'machine', 'learning', 'book'],\n",
    "            ['this', 'is',  'another', 'machine', 'learning', 'book'],\n",
    "            ['one', 'more', 'new', 'book'],\n",
    "        ['this', 'is', 'about', 'machine', 'learning', 'post'],\n",
    "          ['orange', 'juice', 'is', 'the', 'liquid', 'extract', 'of', 'fruit'],\n",
    "          ['orange', 'juice', 'comes', 'in', 'several', 'different', 'varieties'],\n",
    "          ['this', 'is', 'the', 'last', 'machine', 'learning', 'book'],\n",
    "          ['orange', 'juice', 'comes', 'in', 'several', 'different', 'packages'],\n",
    "          ['orange', 'juice', 'is', 'liquid', 'extract', 'from', 'fruit', 'on', 'orange', 'tree']]\n",
    "          \n",
    "import numpy as np\n",
    " \n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return np.asarray(sent_vec) / numw\n",
    " \n",
    "V=[]\n",
    "for sentence in sentences:\n",
    "    V.append(sent_vectorizer(sentence, model))   \n",
    "          \n",
    "     \n",
    "X_train = V[0:6]\n",
    "X_test = V[6:9] \n",
    "Y_train = [0, 0, 0, 0, 1,1]\n",
    "Y_test =  [0,1,1]    \n",
    "     \n",
    "     \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "classifier = MLPClassifier(alpha = 0.7, max_iter=400) \n",
    "classifier.fit(X_train, Y_train)\n",
    " \n",
    "df_results = pd.DataFrame(data=np.zeros(shape=(1,3)), columns = ['classifier', 'train_score', 'test_score'] )\n",
    "train_score = classifier.score(X_train, Y_train)\n",
    "test_score = classifier.score(X_test, Y_test)\n",
    " \n",
    "print  (classifier.predict_proba(X_test))\n",
    "print  (classifier.predict(X_test))\n",
    " \n",
    "df_results.loc[1,'classifier'] = \"MLP\"\n",
    "df_results.loc[1,'train_score'] = train_score\n",
    "df_results.loc[1,'test_score'] = test_score\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
