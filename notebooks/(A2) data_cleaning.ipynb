{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "To build language models on the text corpus we've retrieved from RateMyTeacher.com we need to clean the data. The way we clean data (remove conjunctions, pluralisations, etc...) can have a considerable effect on results. Thus, we want to experiment and clean the data a few different ways to see how the results are affected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import os, sys, path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin by collecting the scraped data and formatting it into datasets**:\n",
    "\n",
    "The `clean_data.gen_data()` method creates the files: \n",
    "- ***review_stats.pbz2*** : a file with all the summary information including scores and teacher info. \n",
    "- ***full_review_text.pbz2*** : a file with all the review texts.\n",
    "\n",
    "<font color=darkred size=1>**No need to run this again if you already have these two files**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from student_voices.clean_data import gen_data\n",
    "# gen_data('D:/Student_Voices_Database/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bin reviews**:\n",
    "\n",
    "The original study binned the ratings into several ranges from lowest to highest. Use the `create_hardcoded_ratings_bins` method to create the same bins. \n",
    "\n",
    "- This will create the file ***by_ratings_range.pbz2*** which is a dictionary of the form `{bin1:[indices],bin2...}`\n",
    "\n",
    "<font color=darkred size=1>**No need to run this again if you already have this file**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from student_voices.clean_data import create_hardcoded_ratings_bins\n",
    "# from student_voices.sv_utils import decompress_pickle\n",
    "\n",
    "# review_data = decompress_pickle('D:/Student_Voices/review_stats.pbz2')\n",
    "# create_hardcoded_ratings_bins(review_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use AWS to Clean the Data\n",
    "\n",
    "Cleaning this ammount of text data can be memory intensive. **This notebook** uses the [spot-connect](https://pypi.org/project/spot-connect/) module to launch virtual machines on AWS to clean the data. \n",
    "\n",
    "**Cleaning the data** means preparing the text data for review by an NLP model. This involves: \n",
    "- Tokenizing \n",
    "- Lemmatizing/Stemming \n",
    "- Removing Stop Words \n",
    "- Removing Numeric Characters \n",
    "- Removing Contractions \n",
    "- and more... \n",
    "\n",
    "A list of preset cleaning parameters can be retrieved with the `clean_data.data_configuration_hardcodes()` command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from student_voices import clean_data\n",
    "data_configurations = clean_data.data_configuration_hardcodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the region and instance AMIs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\users\\computer\\dropbox\\projects\\spot-connect\\spot_connect\\data\\profiles.txt\n"
     ]
    }
   ],
   "source": [
    "from spot_connect import sutils \n",
    "\n",
    "# Change the region for the default profiles\n",
    "sutils.reset_profiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the AWS parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance type \n",
    "instance_type = 'r5.2xlarge'\n",
    "\n",
    "# Number of instances to run \n",
    "n_jobs = 1\n",
    "\n",
    "# Number of physical cores in the instance type \n",
    "n_cores = 4\n",
    "\n",
    "# File system to connect to \n",
    "filesystem = 'student_data'\n",
    "\n",
    "# Region\n",
    "region='us-east-2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uploading the data to AWS**: \n",
    "\n",
    "If you have followed all the steps to use AWS with the `spot-connect` module then you should be able to use the AWS command line interface (awscli). Open a command prompt and type the following command to upload your data to an S3 drive: \n",
    "\n",
    "`aws s3 sync <local_folder> <s3-bucket>`\n",
    "\n",
    "This will upload every file and folder in `<local_folder>` to the S3 bucket you choose which should have the name formatted as `s3://<bucket_name>`.\n",
    "\n",
    "In the my case, this command was: \n",
    "\n",
    "`aws s3 sync D:Student_Voices_Database s3://student-voices` \n",
    "\n",
    "S3 storage is very affordable so once you've uploaded your data feel free to leave it on there.\n",
    "\n",
    "Once the data is on S3, use the `LinkAWS` class to create an instance and connect it to a new or existing elastic file system (EFS), then download the data from S3 to the EFS via the instance. The instance will terminate automatically once the transfer is complete. \n",
    "\n",
    "<font color=blue size=1>Note that the `LinkAWS` class using `awscli` to perform these transfers on the instance which makes it faster than regular FTS transfers<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default key-pair directory is \"C:/Projects/VirtualMachines/Key_Pairs\"\n"
     ]
    }
   ],
   "source": [
    "from spot_connect import instance_manager\n",
    "\n",
    "# Use the LinkAWS to move data and run jobs on AWS \n",
    "aws_link = instance_manager.InstanceManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer data from : <s3 bucket>  to  <folder on instance> using <instance profile access> to connect to <efs>   \n",
    "# aws_link.instance_s3_transfer('s3://student_reviews', '/home/ec2-user/efs/', 'ec2_s3_access', efs='student-reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a monitor instance to upload the repo to the EFS**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a very low cost instance to download the github repo for the project onto the EFS \n",
    "# aws_link.launch_monitor()\n",
    "# aws_link.update_repo(aws_link.monitor, \n",
    "#                      '/home/ec2-user/efs/', \n",
    "#                      branch='master', \n",
    "#                      repo_link='https://github.com/losDaniel/Student-Voices.git')\n",
    "# aws_link.terminate_monitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the job scripts for each instance**:\n",
    "\n",
    "Since we're working with a python module that connects to a linux instance that will then run a python script that needs specific arguments, passing arguments can get complicated. One easy way to get around this is by creating methods that take the arguments you need and generate the bash scripts you need as \"\\n\" separated strings to be run on the instances.\n",
    "\n",
    "In the example below, create one script for each data cleaning configuration we want to apply because we will be using one instance per configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...EFS file system already exists\n",
      "Waiting for availability......Available\n",
      "...EFS file system already exists\n",
      "Waiting for availability......Available\n",
      "...EFS file system already exists\n",
      "Waiting for availability......Available\n"
     ]
    }
   ],
   "source": [
    "from student_voices import ec2_scripts \n",
    "from spot_connect import bash_scripts\n",
    "\n",
    "configs = ['B1','D1','E1']#['A1']#,'B1','C1','D1']\n",
    "\n",
    "n_jobs = len(configs) # number of instances we'll run \n",
    "\n",
    "scripts = [] \n",
    "uploads = [] \n",
    "for config in configs: \n",
    "    script = ec2_scripts.get_instance_setup_script(\n",
    "        filesystem,\n",
    "        region,\n",
    "        run_as_user='ec2-user')\n",
    "\n",
    "    script = ec2_scripts.get_clean_data_script(\n",
    "        config,\n",
    "        'clean_'+config+'_log.txt', \n",
    "        region='us-east-2', \n",
    "        path='/home/ec2-user/efs/data/', \n",
    "        cancel_fleet=True,\n",
    "        run_as_user='ec2-user',\n",
    "        script=script) \n",
    "    \n",
    "    # Convert the working script to base-64 encoded so the fleet can run it \n",
    "    user_data_script = bash_scripts.script_to_userdata(script)\n",
    "\n",
    "    scripts.append(user_data_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key pair KP-data_cleaning_E1 created...\n",
      "Security Group SG-data_cleaning_E1 Created...Key pair detected, re-using...\n",
      "Security group detected, re-using...\n",
      "Key pair detected, re-using...\n",
      "Security group detected, re-using...\n"
     ]
    }
   ],
   "source": [
    "account_number_file = 'C:/Users/Computer/Documents/AWS/account_number.txt'\n",
    "account_num = open(account_number_file).read()\n",
    "        \n",
    "aws_link.run_distributed_jobs(account_num,\n",
    "                              'data_cleaning_'+config,            # Instance prefix \n",
    "                              n_jobs,                             # Number of jobs \n",
    "                              instance_type,                      # Instance type to use\n",
    "                              availability_zone='us-east-2c',\n",
    "                              user_data=scripts,                  # List of scripts, 1 for each job \n",
    "                              instance_profile='instance_manager') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download cleaned data (optional)**:\n",
    "\n",
    "The cleaned data is now on the EFS. Since the language models will run on AWS instances as well there is no need to download the data. However, if you choose to do so simply upload the cleaned data to S3 and then download locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer data from : <s3 bucket>  to  <folder on instance> using <instance profile access> to connect to <efs>   \n",
    "aws_link.instance_s3_transfer('/home/ec2-user/efs/cleaned_data', 's3://student_reviews', 'ec2_s3_access', efs='reviews_efs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this transfer is complete the data will be on S3 and can be downloaded locally using: \n",
    "\n",
    "`aws s3 sync <s3-bucket> <local_folder>`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
